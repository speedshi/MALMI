#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Jul 22 16:45:23 2021

@author: Peidong SHI
@email: speedshi@hotmail.com
"""


import os
import h5py
import datetime
import numpy as np
from ioformatting import vector2trace, output_seissegment
import copy
import obspy
import glob
from obspy import UTCDateTime


# define some common parameters that will be used accross different functions
pbfname_tag = 'prediction_probabilities'  # the filename (no suffix) of probability file
pbfname_EQT = 'prediction_probabilities.hdf5'  # the common filename of probability file for each station
dtformat_EQT = '%Y-%m-%dT%H:%M:%S.%fZ'  # the datetime format using in EQT probability hdf5 outputs
data_size_EQT = 6000  # the data segment size in date points, by default the EQT output are 1 minutes long, thus 6000 points
dt_EQT = 0.01  # time sampling rate of data in second, for EQT probability output, by default is 0.01 s
data_sglength_EQT = (data_size_EQT-1)*dt_EQT  # data segment length in second, equals 'endtime - starttime'


def eqt_arrayeventdetect(dir_probinput, dir_output, sttd_max, twlex, d_thrd, nsta_thrd=3, spttdf_ssmax=None):
    """
    This function is used to detect potential locatable events using the probabilities 
    generated by EQ-transformer from the whole array. The detection probability, 
    P-phase probability, S-phase probability of the detected event segments are 
    output to MSEED format in the output directory.
    
    Parameters
    ----------
    dir_probinput : str
        path to the EQT probability data set of different stations.
    dir_output : str
        path for data outputs.
    sttd_max : float
        maximum P-P traveltime difference between different stations for 
        the whole imaging area, in second.
    twlex : float
        time in second for extending the time window, roughly equal to the 
        width of P- or S-probability envelope. Value ranges from 0.5-2 second.
    d_thrd : float
        detection threshold. The threshold value depends on the performance of 
        the machine learning model. Use a low threshold to detect more weak events,
        but could also increase false positives. Use a high threshold to detect
        events with more confidance, but could miss some weak or usual events.
        Since we will perform migration on phase probabilities to image and 
        locate the souces, we are using spatial coherency accross the whole array.
        So it is suggestted to use a relatively low threshold for detection, 
        and migration process can later take care of event location and 
        potentially remove false positives by additional event selection process
        later. For EQT, suggestted threshold values from 0.05 - 0.5
    nsta_thrd : int, optional
        minimal number of stations triggered during a specified time period, 
        If there are more triggered stations than this threshold, the algrithem
        determines there is an event in the current searched time period, then
        it will start to ouput probapility data of different stations to the 
        defined output directory. Each event will have a unique folder name 
        according to the starttime of its data segment.
        The default is 3.
    spttdf_ssmax: float, optional
        the maximal P to S arrivaltime difference for a perticular station in 
        second for the imaging area. No need to be very accurate.

    Returns
    -------
    Obspy trace data outputted in MSEED format in the defined output directory.

    """

    if spttdf_ssmax is None:
        spttdf_ssmax = 0.5*sttd_max

    # internal parameters
    N_tit = 1  # number of iteration for settle the starttime and endtime of data segment; should be 1, 2, 3; set 1 for fast calculation, set 2 or 3 if want to obtain better time constrain
    
    datainfo = {}
    datainfo['dt'] = dt_EQT
    
    # load timing info and the detection probability
    # obtain the folder name for the results of each station, each folder contain the probability data of one station
    db = {}  # for storing the whole data set: timestamp info + detection probability
    stanames = []
    dirnames = sorted([fdname for fdname in os.listdir(dir_probinput) if os.path.isdir(os.path.join(dir_probinput, fdname))])
    dsg_sttmin = None  # earliest starttime of data segment
    dsg_sttmax = None  # latest endtime of data segment
    for sfdname in dirnames:
        # loop over each station folder, read data set for each station
        station_name = sfdname.split('_')[0]  # the current station name
        pbfile = os.path.join(dir_probinput, sfdname, pbfname_EQT)  # the filename of picking probability for the current station
        
        # load probability data set
        pbdf = h5py.File(pbfile, 'r')
        dsg_name = list(pbdf['probabilities'].keys())  # get the name of each probability data segment 
        dsg_starttime = np.array([datetime.datetime.strptime(idsgnm.split('_')[-1], dtformat_EQT) for idsgnm in dsg_name])  # get the starttime of each probability data segment 
        dsg_endtime = np.array([iitime + datetime.timedelta(seconds=data_sglength_EQT) for iitime in dsg_starttime])  # get the endtime of each probability data segment 
    
        # find the minimal starttime and maximum endtime of all data segments over all stations
        if dsg_sttmin:
            dsg_sttmin = min(dsg_sttmin, min(dsg_starttime))
        else:
            dsg_sttmin = min(dsg_starttime)
        if dsg_sttmax:
            dsg_sttmax = max(dsg_sttmax, max(dsg_endtime))
        else:
            dsg_sttmax = max(dsg_endtime)
        
        prob_D = []
        for idsg in dsg_name:
            pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
            pbdf['probabilities'][idsg].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
            prob_D.append(pbdata[:,0])  # detection probability
            del pbdata
            
        db[station_name] = [dsg_starttime, dsg_endtime, prob_D, dsg_name]  # starting datetime of each data segement and the corresponding detection probability
        stanames.append(station_name)  # all avaliable station names
            
        del station_name, pbfile, pbdf, dsg_name, dsg_starttime, dsg_endtime, prob_D
        
    
    # scan data from 'dsg_sttmin' to 'dsg_sttmax' to search for all potential events/triggers
    # tt1 : the starttime of searched time range
    # tt2 : the endtime of searched time range
    # tts : the starttime of data extraction
    # ttd : the endtime for data extraction, ttd <= tt2
    # tts_sta : the starttime for a probability data segment above threshold at different stations
    # ttd_sta : the endtime for a probability data segment above threshold at different stations
    tt1 = copy.deepcopy(dsg_sttmin)
    ttd_previous = copy.deepcopy(dsg_sttmin)  # the endtime of data extraction for the previous data output
    while tt1 <= dsg_sttmax:
        # Find if there are enough stations have detection values above the 
        # threshold (triggered) at the searched time range. 
        # If yes, then find a event and output data.
        
        # set the endtime for searched time range
        tt2 = tt1 + datetime.timedelta(seconds=(sttd_max+twlex))  # use 'maximum P2S traveltime difference' + 'extend window length' to set the endtime of searching time range
        # make sure tt2 does not exceed the maximum time
        if tt2 > dsg_sttmax:
            tt2 = copy.deepcopy(dsg_sttmax)
        
        for itit in range(N_tit):
            
            if itit > 0:
                tt1 = copy.deepcopy(tts)
                tt2 = copy.deepcopy(ttd)
            
            # initialize parameters
            tts = None
            ttd = None
            tts_sta = {}
            ttd_sta = {}
            nsta_trig = 0  # number of stations triggered
            
            for sta in stanames:
                # loop over each station
                
                # flag to indicate if the current station has already been triggered or not
                station_triggered = False
                
                # maximum detection probability for the current event in the searched time period
                prob_det_max = 0.0
                
                # find all data segments which contain the whole searched time period
                dindx = np.logical_and((db[sta][0] <= tt1), (db[sta][1] >= tt2))  # the index of data segments that include the whole searched time period
                if dindx.any():
                    for isgindex in np.flatnonzero(dindx):
                        # loop over each fulfilled data segment, find the earliest 'tts' and the latest 'ttd'
                        
                        data_sgindex = copy.deepcopy(isgindex)  # the index of the chosen data segment, is an integer
                        data_starttime = db[sta][0][data_sgindex]  # starttime of the chosen data segment
                        data_times = np.array([data_starttime + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point
                        data_probD = db[sta][2][data_sgindex]  # detection probability of the chosen data segment
                        
                        data_pdindex = np.logical_and((data_times >= tt1), (data_times <= tt2))  # the index of probability data point within the detection time range
                        detecid = (data_probD[data_pdindex] >= d_thrd)  # boolen array to indicate whether there are detections above threshold
                        
                        if detecid.any():
                            # have detetion at the current station and the searched time period
                            
                            # determine if this station has been triggered and update the accumulated number
                            if not station_triggered:
                                nsta_trig = nsta_trig + 1  
                                station_triggered = True
                        
                            idfirst = np.flatnonzero(data_pdindex)[0]  # the index of the first point in the searched time period
                            idlast = np.flatnonzero(data_pdindex)[-1]  # the index of the last point in the searched time period
                            
                            # set tts, and update tt2
                            if (data_probD[idfirst] >= d_thrd) and (idfirst > 0) and (data_probD[idfirst-1] >= d_thrd):
                                # starttime and the data point just before the starttime are both above threshold
                                ddinx = np.flatnonzero(data_probD[0:idfirst] < d_thrd)  
                                if ddinx.size > 0:
                                    # get the last occurance for the prior points with a detection value smaller than threshold
                                    tts_temp = data_times[ddinx[-1] + 1] - datetime.timedelta(seconds=twlex)
                                else:
                                    # all the prior data points exceed detection threshold
                                    tts_temp = data_times[0] - datetime.timedelta(seconds=spttdf_ssmax)  # note move the starttime ahead 
                                    
                                del ddinx

                            elif (data_probD[idfirst] >= d_thrd) and (idfirst == 0):
                                # starttime is above the threshold and also is the first point of this segment
                                tts_temp = data_times[0] - datetime.timedelta(seconds=spttdf_ssmax)  # note move the starttime ahead
                            else:
                                # the starttime tt1 has detetion probability below threshold
                                tts_temp = data_times[idfirst + np.argmax(detecid)] - datetime.timedelta(seconds=twlex)
                            
                            # set tts_sta for the current station
                            dprobD_max = max(data_probD[data_pdindex])  # maximum detection probability for the current time segment and station
                            if (dprobD_max > prob_det_max):
                                tts_sta[sta] = copy.deepcopy(tts_temp)
                            # if sta in tts_sta:
                            #     tts_sta[sta] = min(tts_sta[sta], tts_temp)
                            # else:
                            #     tts_sta[sta] = copy.deepcopy(tts_temp)
                            if tts_sta[sta] < ttd_previous:
                                tts_sta[sta] = copy.deepcopy(ttd_previous)
                            
                            # set tts
                            if tts:
                                tts = min(tts, tts_sta[sta])  # tts = min(tts, tts_temp)
                            else:
                                tts = copy.deepcopy(tts_sta[sta])  # tts = copy.deepcopy(tts_temp)
                            # make sure the 'tts' is not earlier than the endtime of the previous data extraction
                            if tts < ttd_previous:
                                tts = copy.deepcopy(ttd_previous)
                            
                            # set tt2
                            if tts > tt1:
                                tt2 = tts + datetime.timedelta(seconds=(sttd_max+twlex))
                            else:
                                tt2 = tt1 + datetime.timedelta(seconds=(sttd_max+twlex))
                            if tt2 > dsg_sttmax:
                                tt2 = copy.deepcopy(dsg_sttmax)
                            
                            del tts_temp
                            
                            # set ttd, and update tt2
                            if (data_probD[idlast] >= d_thrd) and (idlast < data_size_EQT-1) and (data_probD[idlast+1] >= d_thrd):
                                # endtime and the next point of endtime are both above threshold
                                ddinx = np.argmax(data_probD[idlast+1:] < d_thrd)  # first occurance for the remaining points with a detection value smaller than threshold
                                if ddinx > 0:
                                    # the remaining data points have detection value below threshold
                                    ttd_temp = data_times[idlast + ddinx] + datetime.timedelta(seconds=twlex)
                                else:
                                    # all the remaining data points exceed detection threshold
                                    ttd_temp = data_times[-1] + datetime.timedelta(seconds=spttdf_ssmax)  # note move the endtime after
                                    
                                del ddinx

                            elif (data_probD[idlast] >= d_thrd) and (idlast == data_size_EQT-1):
                                # endtime is above the threshold and also is the last point of this segment
                                ttd_temp = data_times[-1] + datetime.timedelta(seconds=spttdf_ssmax)  # note move the endtime after
                            else:
                                # the next point after endtime is below threshold,
                                # or just before or at the endtime is below threshold. 
                                ttd_temp = data_times[idfirst + np.flatnonzero(detecid)[-1]] + datetime.timedelta(seconds=twlex)
                            
                            # set ttd_sta for the current station
                            if (dprobD_max > prob_det_max):
                                ttd_sta[sta] = copy.deepcopy(ttd_temp)
                                prob_det_max = copy.deepcopy(dprobD_max)
                            # if sta in ttd_sta:
                            #     ttd_sta[sta] = max(ttd_sta[sta], ttd_temp)
                            # else:
                            #     ttd_sta[sta] = copy.deepcopy(ttd_temp)
                            if ttd_sta[sta] > dsg_sttmax:
                                ttd_sta[sta] = copy.deepcopy(dsg_sttmax)
                            
                            # set ttd
                            if ttd:
                                ttd = max(ttd, ttd_sta[sta])  # ttd = max(ttd, ttd_temp)
                            else: 
                                ttd = copy.deepcopy(ttd_sta[sta])  # ttd = copy.deepcopy(ttd_temp)
                            if ttd > dsg_sttmax:
                                ttd = copy.deepcopy(dsg_sttmax)
                            
                            # set tt2
                            tt2 = max(tt2, ttd)
                            if tt2 > dsg_sttmax:
                                tt2 = copy.deepcopy(dsg_sttmax)    
                            
                            del idfirst, idlast, ttd_temp, dprobD_max
                            
                        # clear memory
                        del data_sgindex, data_starttime, data_times, data_probD, data_pdindex, detecid

                    del isgindex

                del dindx, prob_det_max
        
            if (nsta_trig < nsta_thrd):
                break
        
        # write P- and S-phase probability data for the current searched time period
        # if there are more triggered stations than the threshold (3 stations)
        # output data from time range: 'tts' to 'ttd'
        if (nsta_trig >= nsta_thrd):
            # print info
            print('----------------------------------------------------------')
            print('Detect event at time range:', tts, '-', ttd)
            print(nsta_trig, 'stations are triggered.')
            print('Start to output data in this time range.')
            
            # after the previous loop over all stations, the 'tt1', 'tt2', 'tts', 'ttd', 'tts_sta', 'ttd_sta' are now fixed
            dir_output_ev = dir_output + '/' + tts.isoformat()  # output directory for the current event/time_range
            
            for sta in stanames:
                # loop over each station, check data, and load P S probability, and output avaliable data set
                
                # set the midpoint time for the current station
                if sta in tts_sta:
                    # set the midpoint time between the starttime and endtime tailored for the current station
                    tt_mid =  tts_sta[sta] + (ttd_sta[sta] - tts_sta[sta])/2 
                else:
                    # no detection for the current station
                    # set the midpoint between the starttime and endtime of data extraction
                    tt_mid =  tts + (ttd - tts)/2  
                
                dindx = np.logical_and((db[sta][0] <= tts), (db[sta][1] >= ttd))  # the index of data segments that include the whole searched time period
                if dindx.any():
                    # have data segments that fulfill the requirements
                    # find the data segment where the searched time period is mostly around the center
                    mdtimesdf = np.array([ttdfc.total_seconds() for ttdfc in db[sta][0][dindx] + datetime.timedelta(seconds=0.5*data_sglength_EQT) - tt_mid])  # time difference in second between the midpoint of the fulfilled data segments time range and the searched time period
                    data_sgindex = np.flatnonzero(dindx)[np.argmin(abs(mdtimesdf))]  # the index of the chosen data segment, is an integer
                    data_sgname = db[sta][3][data_sgindex]  # the segment name of the chosen data segment
                    data_starttime = db[sta][0][data_sgindex]  # starttime of the chosen data segment
                    data_times = np.array([data_starttime + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point for the chosen data segment
                    data_pdindex = np.logical_and((data_times >= tts), (data_times <= ttd))  # the index of probability data point within the detection time range
                    odata_time = data_times[data_pdindex]  # the timestampe of output data
                    
                    # set data info               
                    datainfo['station_name'] = sta
                    datainfo['starttime'] = odata_time[0]  # the starttime of the output data
                    
                    # load data set: Detetion, P and S probability
                    pbfile = os.path.join(dir_probinput, sta+'_outputs', pbfname_EQT)  # the filename of picking probability for the current station
                    pbdf = h5py.File(pbfile, 'r')
                    pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
                    pbdf['probabilities'][data_sgname].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
                    oprob_D = pbdata[data_pdindex,0]  # detection probability
                    oprob_P = pbdata[data_pdindex,1]  # P-phase picking probability
                    oprob_S = pbdata[data_pdindex,2]  # S-phase picking probability
                                    
                    # output detection probability
                    datainfo['channel_name'] = 'PBD'  # note maximum three characters, the last one must be 'D'
                    vector2trace(datainfo, oprob_D, dir_output_ev)
                    
                    # output P-phase picking probability
                    datainfo['channel_name'] = 'PBP'  # note maximum three characters, the last one must be 'P'
                    vector2trace(datainfo, oprob_P, dir_output_ev)
                    
                    # output S-phase picking probability
                    datainfo['channel_name'] = 'PBS'  # note maximum three characters, the last one must be 'S'
                    vector2trace(datainfo, oprob_S, dir_output_ev)
                    
                    # clear memory
                    del mdtimesdf, data_sgindex, data_sgname, data_starttime, data_times, data_pdindex, odata_time
                    del pbfile, pbdf, pbdata, oprob_D, oprob_P, oprob_S
                
                del dindx

            del tt_mid, dir_output_ev
        
            # updata 'ttd_previous'
            ttd_previous = copy.deepcopy(ttd)
        
        # update the starttime for detection
        if ttd:
            tt1 = ttd + datetime.timedelta(seconds=dt_EQT)
        else:
            tt1 = tt2 + datetime.timedelta(seconds=dt_EQT)

        del tts_sta, ttd_sta, tts, ttd, nsta_trig
  
    return    
        
        
def phasedetectfprob(dir_probinput, P_thrd, S_thrd, datafname=None):
    """
    This function is used to detect potential event phases using the phase probabilities 
    generated for each station. Phase probabilies (including P and S) 
    above the input threshold are viewed as potential events. Detection is performed 
    individually on each station.
    
    Parameters
    ----------
    dir_probinput : str
        path to the probability dataset of different stations.
    P_thrd : float
        detection threshold for P-phase. The threshold value depends on the performance of 
        the machine learning model. Use a low threshold to detect more weak events,
        but could also increase false positives. Use a high threshold to detect
        events with more confidance, but could miss some weak or usual events.
        Since we will perform migration on phase probabilities to image and 
        locate the souces, we are using spatial coherency accross the whole array.
        So it is suggestted to use a relatively low threshold for detection, 
        and migration process can later take care of event location and 
        potentially remove false positives by additional event selection process
        later. For EQT conservative model, suggestted threshold values from 0.01 - 0.3
    S_thrd : float
        detection threshold for S-phase. When S-phase probabilities are larger
        than this threshold value, a phase will be detected.  
    datafname : str
        filename of the input phase probability data.
        EQT default probability file: 'prediction_probabilities.hdf5'
        seisbench default probability file: 'prediction_probabilities.mseed'
        default is None, i.e. automatically look for what is avaliable.

    Returns
    -------
    event_info : dictionary
        containing the detected event information at each station;
        Note: station_name should be the identity of a station: 'network.station.location.instrument', 
        need to fix and work on this!
        event_info[station_name][filename]: str indicating the filename of probability 
                                            dataset for the current station;
        event_info[station_name][p][starttime]: a list of datetime indicating the 
                                                starttime of each event detected 
                                                on the P-phase probability;
        event_info[station_name][P][endtime]: a list of datetime indicating the 
                                              endtime of each event detected 
                                              on the P-phase probability;
        event_info[station_name][P][mxptime]: a list of datetime indicating the 
                                              maximum probability time of each event 
                                              detected on the P-phase probability,
                                              is the picked arrivaltime for P-phase;
        event_info[station_name][P][maxprob]: a list of float indicating the maximum
                                              P-phase probability between P-starttime
                                              and P-endtime of each detected event;
        event_info[station_name][P][sgname]: a list of str indicating the P-probability
                                             data segment name for extracting maxprob;                              
        event_info[station_name][S][starttime]: a list of datetime indicating the 
                                                starttime of each event detected 
                                                on the S-phase probability;
        event_info[station_name][S][endtime]: a list of datetime indicating the 
                                              endtime of each event detected 
                                              on the S-phase probability;
        event_info[station_name][S][mxptime]: a list of datetime indicating the 
                                              maximum probability time of each event 
                                              detected on the S-phase probability,
                                              is the picked arrivaltime for S-phase;
        event_info[station_name][S][maxprob]: a list of float indicating the maximum
                                              S-phase probability between S-starttime
                                              and S-endtime of each detected event;
        event_info[station_name][S][sgname]: a list of str indicating the S-probability
                                             data segment name for extracting maxprob; 

    """
    
    event_info = {}  # initialize
    
    # obtain the folder name for the results of each station, each folder contain the probability data of one station
    dirnames = sorted([fdname for fdname in os.listdir(dir_probinput) if os.path.isdir(os.path.join(dir_probinput, fdname))])  # directory names for each station
    for sfdname in dirnames:
        # loop over each station folder, read data set for each station
        station_name = sfdname.split('_')[0]  # the current station name, station_name should be the identity of a station: 'network.station.location.instrument', need to fix and work on this
        
        if isinstance(datafname, str):
            pbfile = os.path.join(dir_probinput, sfdname, datafname)  # the filename of picking probability for the current station
            dataformat = datafname.split('.')[-1].lower()
        elif datafname is None:
            pbfile_lists = glob.glob(os.path.join(dir_probinput, sfdname, pbfname_tag+'*'), recursive=True)
            if len(pbfile_lists) == 1:
                pbfile = pbfile_lists[0]
                dataformat = pbfile.split('.')[-1].lower()
            else:
                raise ValueError('Cannot find correct probability file: {}!'.format(pbfile_lists))
        
        # initialize
        event_info[station_name] = {}
        event_info[station_name]['filename'] = copy.deepcopy(pbfile)  # record the filename of the probability dataset
        event_info[station_name]['P'] = {}
        event_info[station_name]['P']['starttime'] = []
        event_info[station_name]['P']['endtime'] = []
        event_info[station_name]['P']['mxptime'] = []
        event_info[station_name]['P']['maxprob'] = []
        event_info[station_name]['P']['sgname'] = []
        event_info[station_name]['S'] = {}
        event_info[station_name]['S']['starttime'] = []
        event_info[station_name]['S']['endtime'] = []
        event_info[station_name]['S']['mxptime'] = []
        event_info[station_name]['S']['maxprob'] = []
        event_info[station_name]['S']['sgname'] = []
        
        # load probability data set and the timing information
        if dataformat == 'hdf5':
            # inputs are h5 data in the form of EQT phase probability outputting style
            pbdf = h5py.File(pbfile, 'r')
            dsg_name = list(pbdf['probabilities'].keys())  # get the names of all probability data segments 
            dsg_starttime = np.array([datetime.datetime.strptime(idsgnm.split('_')[-1], dtformat_EQT) for idsgnm in dsg_name])  # get the starttimes of all probability data segments 
            dsg_endtime = np.array([iitime + datetime.timedelta(seconds=data_sglength_EQT) for iitime in dsg_starttime])  # get the endtimes of all probability data segments
            data_size = data_size_EQT
        else:
            # inputs are MSEED format which can be read by obspy
            pbdf = obspy.read(pbfile)
            pbdf.merge(method=1, fill_value=0, interpolation_samples=-1)
            dsg_name = [None,]
            dsg_starttime = np.array([pbdf[0].stats.starttime,])      
            dsg_endtime = np.array([pbdf[0].stats.endtime,])
            data_size = pbdf[0].stats.npts
            
        for ii, idsg in enumerate(dsg_name):
            # loop over each data segment
            pbdata = np.zeros((data_size, 3), dtype=np.float32)  # initialize an array for loading prob data set
            if dataformat == 'hdf5':
                pbdf['probabilities'][idsg].read_direct(pbdata)  # EQT probability data set, shape: 6000*3, colume 0: event_prob; colume 1: P_prob; colume 2: S_prob
                pbtime = [dsg_starttime[ii] + datetime.timedelta(seconds=ipoint*dt_EQT) for ipoint in range(data_size)]  # datetime of each data point
            else:
                for iitr in pbdf:
                    assert(iitr.stats.starttime == dsg_starttime[ii])
                    assert(iitr.stats.endtime == dsg_endtime[ii])
                    if iitr.stats.channel[-1].upper() == 'P':
                        pbdata[:,1] = iitr.data  # P_prob
                    elif iitr.stats.channel[-1].upper() == 'S':
                        pbdata[:,2] = iitr.data  # S_prob
                    else:
                        pbdata[:,0] = iitr.data  # detetion or noise probability
                pbtime = [dsg_starttime[ii]+jdtt for jdtt in pbdf[0].times()]
                
            # perform detection based on P-phase probabilites: pbdata[:,1]
            epindx = np.flatnonzero((pbdata[:,1] >= P_thrd))  # the indices of all data points with probability larger than threshold
            for iep in epindx:
                if (iep == 0) or ((iep-1) not in epindx):
                    # current time is the starttime of a detection
                    temp_startime = copy.deepcopy(pbtime[iep])  # get the starttime of the detected event
                    start_did = copy.deepcopy(iep)  # the data index of the starting point of the detected event
                if (iep == (data_size-1)) or ((iep+1) not in epindx):
                    # current time is the endtime of a detection
                    temp_endtime = copy.deepcopy(pbtime[iep])  # get the endtime of the detected event
                    end_did = copy.deepcopy(iep)  # the data index of the ending point of the detected event
                    temp_maxprob = max(pbdata[start_did:end_did+1,1])  # get the maximum probability of the detected event
                    temp_mxptime = pbtime[start_did + np.argmax(pbdata[start_did:end_did+1,1])]  # get the maximum probability time of the detected event, i.e. phase-picking time
                    assert((temp_mxptime >= temp_startime) and (temp_mxptime <= temp_endtime))
                                   
                    # compare with the existing detected events
                    if event_info[station_name]['P']['starttime']:
                        # there are existing events in the event list
                        # check with existing events, get the index of the events that need to update info
                        
                        # situation when need to update starttime of one event
                        # maximum only one event need to update!
                        # exist: |-------------++++++++++-------------|
                        # temp : |-----------++++++++++++-------------|
                        evid_s = np.flatnonzero((np.array(event_info[station_name]['P']['starttime']) > temp_startime)
                                                & (np.array(event_info[station_name]['P']['starttime']) <= temp_endtime)
                                                & (np.array(event_info[station_name]['P']['endtime']) >= temp_endtime))
                        assert(len(evid_s) <= 1), "More than one event to only update starttime. Impossible. There must be errors!"
                        
                        # situation when need to update endtime of one event
                        # maximum only one event need to update!
                        # exist: |-------------++++++++++-------------|
                        # temp : |-------------+++++++++++------------|
                        evid_e = np.flatnonzero((np.array(event_info[station_name]['P']['starttime']) <= temp_startime) 
                                                & (np.array(event_info[station_name]['P']['endtime']) >= temp_startime)
                                                & (np.array(event_info[station_name]['P']['endtime']) < temp_endtime))
                        assert(len(evid_e) <= 1), "More than one event to only update endtime. Impossible. There must be errors!"
                        
                        # situation when need to update both starttime and endtime
                        # might be more than one event to update!
                        # exist: |-------------++++++++++-------------|
                        # temp : |-----------+++++++++++++------------|
                        evid_se = np.flatnonzero((np.array(event_info[station_name]['P']['starttime']) > temp_startime) 
                                                 & (np.array(event_info[station_name]['P']['endtime']) < temp_endtime))
                        
                        # situation when detected event time range is within the time range of one event
                        # no need to update starttime and endtime, but need to check if update 'maxprob' and 'sgname'
                        # maximum only one event need to update!
                        # exist: |-------------++++++++++-------------|
                        # temp : |-------------++++++++++-------------|
                        evid_in = np.flatnonzero((np.array(event_info[station_name]['P']['starttime']) <= temp_startime) 
                                                 & (np.array(event_info[station_name]['P']['endtime']) >= temp_endtime))
                        assert(len(evid_in) <= 1), "More than one event containing the detected event. Impossible. There must be errors!"
                        
                        if (len(evid_in) == 1):
                            # the detected event starttime and endtime are within the starttime and endtime of one event in the list
                            # no need to update starttime and endtime, but need to check if update 'maxprob' and 'sgname'
                            assert(len(evid_s)==0 and len(evid_e)==0 and len(evid_se)==0)
                            if event_info[station_name]['P']['maxprob'][evid_in[0]] < temp_maxprob:
                                # need to update 'maxprob', 'mxptime' and 'sgname'
                                event_info[station_name]['P']['maxprob'][evid_in[0]] = copy.deepcopy(temp_maxprob)
                                event_info[station_name]['P']['mxptime'][evid_in[0]] = copy.deepcopy(temp_mxptime)
                                event_info[station_name]['P']['sgname'][evid_in[0]] = copy.deepcopy(idsg)
                                
                        elif (len(evid_s) + len(evid_e) + len(evid_se) == 1):
                            # only one event in the event list need to update
                            assert(len(np.concatenate((evid_s, evid_e, evid_se))) == 1)
                            evid = np.concatenate((evid_s, evid_e, evid_se))[0]  # the index of the event to update
                            event_info[station_name]['P']['starttime'][evid] = copy.deepcopy(min(event_info[station_name]['P']['starttime'][evid], temp_startime))
                            event_info[station_name]['P']['endtime'][evid] = copy.deepcopy(max(event_info[station_name]['P']['endtime'][evid], temp_endtime))
                            if event_info[station_name]['P']['maxprob'][evid] < temp_maxprob:
                                event_info[station_name]['P']['maxprob'][evid] = copy.deepcopy(temp_maxprob)
                                event_info[station_name]['P']['mxptime'][evid] = copy.deepcopy(temp_mxptime)
                                event_info[station_name]['P']['sgname'][evid] = copy.deepcopy(idsg)
                            del evid
                            
                        elif (len(evid_s) + len(evid_e) + len(evid_se) > 1):
                            # more than one event in the event list are overlaping with the detected event
                            # update the first event to include all, and remove the other events from event list
                            evid = np.sort(np.concatenate((evid_s, evid_e, evid_se)))  # the sorted index array of the events which show overlaping times with the detected event
                            assert(len(evid) > 1)
                            event_info[station_name]['P']['starttime'][evid[0]] = copy.deepcopy(min(np.min(np.array(event_info[station_name]['P']['starttime'])[evid]), temp_startime))
                            event_info[station_name]['P']['endtime'][evid[0]] = copy.deepcopy(max(np.max(np.array(event_info[station_name]['P']['endtime'])[evid]), temp_endtime))
                            xxid = np.argmax(np.array(event_info[station_name]['P']['maxprob'])[evid])
                            if event_info[station_name]['P']['maxprob'][evid[xxid]] < temp_maxprob:
                                event_info[station_name]['P']['maxprob'][evid[0]] = copy.deepcopy(temp_maxprob)
                                event_info[station_name]['P']['mxptime'][evid[0]] = copy.deepcopy(temp_mxptime)
                                event_info[station_name]['P']['sgname'][evid[0]] = copy.deepcopy(idsg)
                            else:
                                event_info[station_name]['P']['maxprob'][evid[0]] = copy.deepcopy(event_info[station_name]['P']['maxprob'][evid[xxid]])
                                event_info[station_name]['P']['mxptime'][evid[0]] = copy.deepcopy(event_info[station_name]['P']['mxptime'][evid[xxid]])
                                event_info[station_name]['P']['sgname'][evid[0]] = copy.deepcopy(event_info[station_name]['P']['sgname'][evid[xxid]])
                            for jjj in evid[-1:0:-1]:
                                # delete the other events
                                del event_info[station_name]['P']['starttime'][jjj]
                                del event_info[station_name]['P']['endtime'][jjj]
                                del event_info[station_name]['P']['maxprob'][jjj]
                                del event_info[station_name]['P']['mxptime'][jjj]
                                del event_info[station_name]['P']['sgname'][jjj]
                            del evid, xxid
                            
                        else:
                            # do not have time overlaping with any event, directly insert into the event list
                            assert(len(evid_s)==0 and len(evid_e)==0 and len(evid_se)==0 and len(evid_in) ==0)
                            insertid = np.flatnonzero(np.array(event_info[station_name]['P']['starttime']) > temp_startime)
                            if (len(insertid) > 0):
                                event_info[station_name]['P']['starttime'].insert(insertid[0], copy.deepcopy(temp_startime))
                                event_info[station_name]['P']['endtime'].insert(insertid[0], copy.deepcopy(temp_endtime))
                                event_info[station_name]['P']['maxprob'].insert(insertid[0], copy.deepcopy(temp_maxprob))
                                event_info[station_name]['P']['mxptime'].insert(insertid[0], copy.deepcopy(temp_mxptime))
                                event_info[station_name]['P']['sgname'].insert(insertid[0], copy.deepcopy(idsg))
                            else:
                                event_info[station_name]['P']['starttime'].append(copy.deepcopy(temp_startime))
                                event_info[station_name]['P']['endtime'].append(copy.deepcopy(temp_endtime))
                                event_info[station_name]['P']['maxprob'].append(copy.deepcopy(temp_maxprob))
                                event_info[station_name]['P']['mxptime'].append(copy.deepcopy(temp_mxptime))
                                event_info[station_name]['P']['sgname'].append(copy.deepcopy(idsg))
                            del insertid

                        del evid_s, evid_e, evid_se, evid_in

                    else:
                        # no detected events yet, append the detected event to the empty event list directly
                        event_info[station_name]['P']['starttime'].append(copy.deepcopy(temp_startime))
                        event_info[station_name]['P']['endtime'].append(copy.deepcopy(temp_endtime))
                        event_info[station_name]['P']['maxprob'].append(copy.deepcopy(temp_maxprob))
                        event_info[station_name]['P']['mxptime'].append(copy.deepcopy(temp_mxptime))
                        event_info[station_name]['P']['sgname'].append(copy.deepcopy(idsg))
                    
                    del temp_startime, temp_endtime, temp_maxprob, temp_mxptime, start_did, end_did
                    assert(event_info[station_name]['P']['mxptime'][-1] >= event_info[station_name]['P']['starttime'][-1])
                    assert(event_info[station_name]['P']['mxptime'][-1] <= event_info[station_name]['P']['endtime'][-1])
            del epindx
            
            # perform detection based on S-phase probabilites: pbdata[:,2]
            epindx = np.flatnonzero((pbdata[:,2] >= S_thrd))  # the indices of all data points with probability larger than threshold
            for iep in epindx:
                if (iep == 0) or ((iep-1) not in epindx):
                    # current time is the starttime of a detection
                    temp_startime = copy.deepcopy(pbtime[iep])  # get the starttime of the detected event
                    start_did = copy.deepcopy(iep)  # the data index of the starting point of the detected event
                if (iep == (data_size-1)) or ((iep+1) not in epindx):
                    # current time is the endtime of a detection
                    temp_endtime = copy.deepcopy(pbtime[iep])  # get the endtime of the detected event
                    end_did = copy.deepcopy(iep)  # the data index of the ending point of the detected event
                    temp_maxprob = max(pbdata[start_did:end_did+1,2])  # get the maximum probability of the detected event
                    temp_mxptime = pbtime[start_did + np.argmax(pbdata[start_did:end_did+1,2])]  # get the maximum probability time of the detected event, i.e. phase-picking time
                    assert((temp_mxptime >= temp_startime) and (temp_mxptime <= temp_endtime))
                                   
                    # compare with the existing detected events
                    if event_info[station_name]['S']['starttime']:
                        # there are existing events in the event list
                        # check with existing events, get the index of the events that need to update info
                        
                        # situation when need to update starttime of one event
                        # maximum only one event need to update!
                        # exist: |-------------++++++++++-------------|
                        # temp : |-----------++++++++++++-------------|
                        evid_s = np.flatnonzero((np.array(event_info[station_name]['S']['starttime']) > temp_startime)
                                                & (np.array(event_info[station_name]['S']['starttime']) <= temp_endtime)
                                                & (np.array(event_info[station_name]['S']['endtime']) >= temp_endtime))
                        assert(len(evid_s) <= 1), "More than one event to only update starttime. Impossible. There must be errors!"
                        
                        # situation when need to update endtime of one event
                        # maximum only one event need to update!
                        # exist: |-------------++++++++++-------------|
                        # temp : |-------------+++++++++++------------|
                        evid_e = np.flatnonzero((np.array(event_info[station_name]['S']['starttime']) <= temp_startime) 
                                                & (np.array(event_info[station_name]['S']['endtime']) >= temp_startime)
                                                & (np.array(event_info[station_name]['S']['endtime']) < temp_endtime))
                        assert(len(evid_e) <= 1), "More than one event to only update endtime. Impossible. There must be errors!"
                        
                        # situation when need to update both starttime and endtime
                        # might be more than one event to update!
                        # exist: |-------------++++++++++-------------|
                        # temp : |-----------+++++++++++++------------|
                        evid_se = np.flatnonzero((np.array(event_info[station_name]['S']['starttime']) > temp_startime) 
                                                 & (np.array(event_info[station_name]['S']['endtime']) < temp_endtime))
                        
                        # situation when detected event time range is within the time range of one event
                        # no need to update starttime and endtime, but need to check if update 'maxprob' and 'sgname'
                        # maximum only one event need to update!
                        # exist: |-------------++++++++++-------------|
                        # temp : |-------------++++++++++-------------|
                        evid_in = np.flatnonzero((np.array(event_info[station_name]['S']['starttime']) <= temp_startime) 
                                                 & (np.array(event_info[station_name]['S']['endtime']) >= temp_endtime))
                        assert(len(evid_in) <= 1), "More than one event containing the detected event. Impossible. There must be errors!"
                        
                        if (len(evid_in) == 1):
                            # the detected event starttime and endtime are within the starttime and endtime of one event in the list
                            # no need to update starttime and endtime, but need to check if update 'maxprob' and 'sgname'
                            assert(len(evid_s)==0 and len(evid_e)==0 and len(evid_se)==0)
                            if event_info[station_name]['S']['maxprob'][evid_in[0]] < temp_maxprob:
                                # need to update 'maxprob', 'mxptime' and 'sgname'
                                event_info[station_name]['S']['maxprob'][evid_in[0]] = copy.deepcopy(temp_maxprob)
                                event_info[station_name]['S']['mxptime'][evid_in[0]] = copy.deepcopy(temp_mxptime)
                                event_info[station_name]['S']['sgname'][evid_in[0]] = copy.deepcopy(idsg)
                                
                        elif (len(evid_s) + len(evid_e) + len(evid_se) == 1):
                            # only one event in the event list need to update
                            assert(len(np.concatenate((evid_s, evid_e, evid_se))) == 1)
                            evid = np.concatenate((evid_s, evid_e, evid_se))[0]  # the index of the event to update
                            event_info[station_name]['S']['starttime'][evid] = copy.deepcopy(min(event_info[station_name]['S']['starttime'][evid], temp_startime))
                            event_info[station_name]['S']['endtime'][evid] = copy.deepcopy(max(event_info[station_name]['S']['endtime'][evid], temp_endtime))
                            if event_info[station_name]['S']['maxprob'][evid] < temp_maxprob:
                                event_info[station_name]['S']['maxprob'][evid] = copy.deepcopy(temp_maxprob)
                                event_info[station_name]['S']['mxptime'][evid] = copy.deepcopy(temp_mxptime)
                                event_info[station_name]['S']['sgname'][evid] = copy.deepcopy(idsg)
                            del evid
                            
                        elif (len(evid_s) + len(evid_e) + len(evid_se) > 1):
                            # more than one event in the event list are overlaping with the detected event
                            # update the first event to include all, and remove the other events from event list
                            evid = np.sort(np.concatenate((evid_s, evid_e, evid_se)))  # the sorted index array of the events which show overlaping times with the detected event
                            assert(len(evid) > 1)
                            event_info[station_name]['S']['starttime'][evid[0]] = copy.deepcopy(min(np.min(np.array(event_info[station_name]['S']['starttime'])[evid]), temp_startime))
                            event_info[station_name]['S']['endtime'][evid[0]] = copy.deepcopy(max(np.max(np.array(event_info[station_name]['S']['endtime'])[evid]), temp_endtime))
                            xxid = np.argmax(np.array(event_info[station_name]['S']['maxprob'])[evid])
                            if event_info[station_name]['S']['maxprob'][evid[xxid]] < temp_maxprob:
                                event_info[station_name]['S']['maxprob'][evid[0]] = copy.deepcopy(temp_maxprob)
                                event_info[station_name]['S']['mxptime'][evid[0]] = copy.deepcopy(temp_mxptime)
                                event_info[station_name]['S']['sgname'][evid[0]] = copy.deepcopy(idsg)
                            else:
                                event_info[station_name]['S']['maxprob'][evid[0]] = copy.deepcopy(event_info[station_name]['S']['maxprob'][evid[xxid]])
                                event_info[station_name]['S']['mxptime'][evid[0]] = copy.deepcopy(event_info[station_name]['S']['mxptime'][evid[xxid]])
                                event_info[station_name]['S']['sgname'][evid[0]] = copy.deepcopy(event_info[station_name]['S']['sgname'][evid[xxid]])
                            for jjj in evid[-1:0:-1]:
                                # delete the other events
                                del event_info[station_name]['S']['starttime'][jjj]
                                del event_info[station_name]['S']['endtime'][jjj]
                                del event_info[station_name]['S']['maxprob'][jjj]
                                del event_info[station_name]['S']['mxptime'][jjj]
                                del event_info[station_name]['S']['sgname'][jjj]
                            del evid, xxid
                            
                        else:
                            # do not have time overlaping with any event, directly insert into the event list
                            assert(len(evid_s)==0 and len(evid_e)==0 and len(evid_se)==0 and len(evid_in) ==0)
                            insertid = np.flatnonzero(np.array(event_info[station_name]['S']['starttime']) > temp_startime)
                            if (len(insertid) > 0):
                                event_info[station_name]['S']['starttime'].insert(insertid[0], copy.deepcopy(temp_startime))
                                event_info[station_name]['S']['endtime'].insert(insertid[0], copy.deepcopy(temp_endtime))
                                event_info[station_name]['S']['maxprob'].insert(insertid[0], copy.deepcopy(temp_maxprob))
                                event_info[station_name]['S']['mxptime'].insert(insertid[0], copy.deepcopy(temp_mxptime))
                                event_info[station_name]['S']['sgname'].insert(insertid[0], copy.deepcopy(idsg))
                            else:
                                event_info[station_name]['S']['starttime'].append(copy.deepcopy(temp_startime))
                                event_info[station_name]['S']['endtime'].append(copy.deepcopy(temp_endtime))
                                event_info[station_name]['S']['maxprob'].append(copy.deepcopy(temp_maxprob))
                                event_info[station_name]['S']['mxptime'].append(copy.deepcopy(temp_mxptime))
                                event_info[station_name]['S']['sgname'].append(copy.deepcopy(idsg))
                            del insertid

                        del evid_s, evid_e, evid_se, evid_in

                    else:
                        # no detected events yet, append the detected event to the empty event list directly
                        event_info[station_name]['S']['starttime'].append(copy.deepcopy(temp_startime))
                        event_info[station_name]['S']['endtime'].append(copy.deepcopy(temp_endtime))
                        event_info[station_name]['S']['maxprob'].append(copy.deepcopy(temp_maxprob))
                        event_info[station_name]['S']['mxptime'].append(copy.deepcopy(temp_mxptime))
                        event_info[station_name]['S']['sgname'].append(copy.deepcopy(idsg))
                    
                    del temp_startime, temp_endtime, temp_maxprob, temp_mxptime, start_did, end_did
                    assert(event_info[station_name]['S']['mxptime'][-1] >= event_info[station_name]['S']['starttime'][-1])
                    assert(event_info[station_name]['S']['mxptime'][-1] <= event_info[station_name]['S']['endtime'][-1])
            del epindx 
            del pbdata, pbtime

        assert(len(event_info[station_name]['P']['starttime']) == len(event_info[station_name]['P']['endtime']))
        assert(len(event_info[station_name]['P']['endtime']) == len(event_info[station_name]['P']['maxprob']))
        assert(len(event_info[station_name]['P']['maxprob']) == len(event_info[station_name]['P']['mxptime']))
        assert(len(event_info[station_name]['P']['mxptime']) == len(event_info[station_name]['P']['sgname']))
        assert(len(event_info[station_name]['S']['starttime']) == len(event_info[station_name]['S']['endtime']))
        assert(len(event_info[station_name]['S']['endtime']) == len(event_info[station_name]['S']['maxprob']))
        assert(len(event_info[station_name]['S']['maxprob']) == len(event_info[station_name]['S']['mxptime']))
        assert(len(event_info[station_name]['S']['mxptime']) == len(event_info[station_name]['S']['sgname']))
        del station_name, pbfile, pbdf, dsg_name, dsg_starttime, dsg_endtime 
    
    return event_info


def arrayeventdetect(event_info, twind_srch, twlex=0.0, nsta_thrd=3, npha_thrd=4, dir_output='./MAILMI_events_prob/', dir_output_seis='./MAILMI_events_seis/', dir_seisdataset=None, seismic_channels=None, output_allsta=True):
    """
    This function is used to detect locatable event accross the whole arrary.
    If there are more triggered stations than the threshold (>=nsta_thrd) 
    and more phase detected than the threshold (>=npha_thrd), the algrithem
    determines there is an event in the current searched time period, then
    it will start to ouput probapility data of different stations to the 
    defined output directory. Each event will have a unique folder name 
    according to the starttime of its data segment.
    Parameters
    ----------
    event_info : dict
        containing the detected event information at each station. Check 
        'phasedetectfprob' for detailed information.
    twind_srch : float
        time window length in second where events will be searched in this range.
        How to determine this parameter:
        Conservative estimation: maximum P-S traveltime difference between 
        different stations for the whole imaging area.
    twlex : float, optional
        time window length in second for extending the output time range, 
        usually set to be 0.5-2 second. The default is 0.0.
    nsta_thrd : int, optional
        minimal number of stations triggered during the specified event time period.
        The default is 3.
    npha_thrd : int, optional
        minimal number of phases triggered during the specified event time period.
        The default is 4.
    dir_output : str, optional
        the output directory for probabilities of each detected event.
    dir_output_seis : str, optional
        the output directory for raw seismic data of each detected event.
    dir_seisdataset : str, optional
        path to the folder where all seismic data are saved. Default is None.
        None means do not output raw seismic data segments.
    seismic_channels : list of str, default is None
        specify the channels of the input seismic data.
    output_allsta : boolen, default is True
        whether output data of all stations or only triggered stations.
        True: output all stations; False: output only the triggered stations.
    
    Returns
    -------
    Obspy trace data outputted in MSEED format in the defined output directory
    for each detected event.

    """
    
    from ioformatting import read_seismic_fromfd
    
    timeformat = "%Y%m%dT%H%M%SZ"
    
    datainfo = {}
    
    stations = list(event_info.keys())  # get all station ids
    
    # make sure output directory exist
    if not os.path.exists(dir_output):
        os.makedirs(dir_output)
    evstaphsfile = os.path.join(dir_output, 'event_station_phase_info.txt')
    if os.path.exists(evstaphsfile):
        fepinfo = open(evstaphsfile, 'a')  # file to record the event trigger and phase information
    else:
        fepinfo = open(evstaphsfile, 'a')  # file to record the event trigger and phase information
        fepinfo.write('# starttime            endtime            station    phase    \n')  # title line
    
    # get the earliest starttime and the latest endtime of all detections for each station
    etime_sta = []  # the earliest starttime of all detections for each station
    ltime_sta = []  # the latest endtime of all detections for each station
    for sta in stations:
        # some stations may not have any detections, need to check
        if (len(event_info[sta]['P']['starttime']) > 0) and (len(event_info[sta]['S']['starttime']) > 0):
            etime_sta.append(copy.deepcopy(min(min(event_info[sta]['P']['starttime']), min(event_info[sta]['S']['starttime']))))
            ltime_sta.append(copy.deepcopy(max(max(event_info[sta]['P']['endtime']), max(event_info[sta]['S']['endtime']))))    
        elif (len(event_info[sta]['P']['starttime']) > 0) and (len(event_info[sta]['S']['starttime']) == 0):
            etime_sta.append(copy.deepcopy(min(event_info[sta]['P']['starttime'])))
            ltime_sta.append(copy.deepcopy(max(event_info[sta]['P']['endtime'])))
        elif (len(event_info[sta]['P']['starttime']) == 0) and (len(event_info[sta]['S']['starttime']) > 0):
            etime_sta.append(copy.deepcopy(min(event_info[sta]['S']['starttime'])))
            ltime_sta.append(copy.deepcopy(max(event_info[sta]['S']['endtime'])))       
    
    if (not etime_sta) and (not ltime_sta):
        return
        
    time_min = copy.deepcopy(min(etime_sta))  # the earliest starttime of all events, used to set the start point of the searched time range
    time_max = copy.deepcopy(max(ltime_sta))  # the latest endtime of all events, used to limit the searched time range
    del etime_sta, ltime_sta, sta
        
    # scan the whole array for locatable events
    detections_all = []
    srchtime_start = copy.deepcopy(time_min)  # use the earliest starttime of all events as the start time for searching
    while srchtime_start <= time_max:
        srchtime_end = copy.deepcopy(srchtime_start + datetime.timedelta(seconds=twind_srch))  # get/update the end of the searched time range
        
        # initialize parameters
        nsta_trig = 0  # total number of stations triggered
        npha_trig = 0  # total number of phases triggered
        etime_sta = []  # for storing the earliest starttime of all the remaining detections after the current round of searching
        output_info = {}  # for storing the output data set information
        etime_amax = None  # the maximum endtime of all detections during the current searched time range
        
        # determin how many stations are triggered in the current searched time range and choose segment name according to event probabilities
        for ista in stations:
            output_info[ista] = {}
            output_info[ista]['P'] = {} 
            output_info[ista]['S'] = {} 
            
            # loop over each station to search for events
            Pevidx = np.flatnonzero((np.array(event_info[ista]['P']['endtime']) >= srchtime_start) & (np.array(event_info[ista]['P']['starttime']) <= srchtime_end))  # index for P detections that are in the current searched time range
            Sevidx = np.flatnonzero((np.array(event_info[ista]['S']['endtime']) >= srchtime_start) & (np.array(event_info[ista]['S']['starttime']) <= srchtime_end))  # index for S detections that are in the current searched time range
            
            if (len(Pevidx) > 0) and (len(Sevidx) > 0):
                # have both P and S detection in this time range
                nsta_trig = nsta_trig + 1  # update total number of stations triggered
                npha_trig = npha_trig + 2  # update total number of phases triggered
                output_info[ista]['P']['detected'] = True  # recored the P phase detection status of this station
                output_info[ista]['S']['detected'] = True  # recored the S phase detection status of this station
                
                mxpeid = np.argmax(np.array(event_info[ista]['P']['maxprob'])[Pevidx])
                output_info[ista]['P']['sgname'] = copy.deepcopy(event_info[ista]['P']['sgname'][Pevidx[mxpeid]])  # the segment name 
                mxseid = np.argmax(np.array(event_info[ista]['S']['maxprob'])[Sevidx])
                output_info[ista]['S']['sgname'] = copy.deepcopy(event_info[ista]['S']['sgname'][Sevidx[mxseid]])  # the segment name 
                
                if etime_amax is None:
                    etime_amax = copy.deepcopy(max(np.max(np.array(event_info[ista]['P']['endtime'])[Pevidx]), np.max(np.array(event_info[ista]['S']['endtime'])[Sevidx])))
                else:
                    etime_amax = copy.deepcopy(max(etime_amax, max(np.max(np.array(event_info[ista]['P']['endtime'])[Pevidx]), np.max(np.array(event_info[ista]['S']['endtime'])[Sevidx]))))
                
                del mxpeid, mxseid
                
            elif (len(Pevidx) > 0) and (len(Sevidx) == 0):
                # only have P detection in this time range
                nsta_trig = nsta_trig + 1  # update total number of stations triggered
                npha_trig = npha_trig + 1  # update total number of phases triggered
                output_info[ista]['P']['detected'] = True  # recored the P phase detection status of this station
                output_info[ista]['S']['detected'] = False  # recored the S phase detection status of this station
                
                mxpeid = np.argmax(np.array(event_info[ista]['P']['maxprob'])[Pevidx])
                output_info[ista]['P']['sgname'] = copy.deepcopy(event_info[ista]['P']['sgname'][Pevidx[mxpeid]])  # the segment name 
                output_info[ista]['S']['sgname'] = copy.deepcopy(output_info[ista]['P']['sgname'])
                
                if etime_amax is None:
                    etime_amax = copy.deepcopy(np.max(np.array(event_info[ista]['P']['endtime'])[Pevidx]))
                else:
                    etime_amax = copy.deepcopy(max(etime_amax, np.max(np.array(event_info[ista]['P']['endtime'])[Pevidx])))
                
                del mxpeid
                
            elif (len(Pevidx) == 0) and (len(Sevidx) > 0):
                # only have S detection in this time range
                nsta_trig = nsta_trig + 1  # update total number of stations triggered
                npha_trig = npha_trig + 1  # update total number of phases triggered
                output_info[ista]['P']['detected'] = False  # recored the P phase detection status of this station
                output_info[ista]['S']['detected'] = True  # recored the S phase detection status of this station

                mxseid = np.argmax(np.array(event_info[ista]['S']['maxprob'])[Sevidx])
                output_info[ista]['S']['sgname'] = copy.deepcopy(event_info[ista]['S']['sgname'][Sevidx[mxseid]])  # the segment name 
                output_info[ista]['P']['sgname'] = copy.deepcopy(output_info[ista]['S']['sgname'])
                
                if etime_amax is None:
                    etime_amax = copy.deepcopy(np.max(np.array(event_info[ista]['S']['endtime'])[Sevidx]))
                else:
                    etime_amax = copy.deepcopy(max(etime_amax, np.max(np.array(event_info[ista]['S']['endtime'])[Sevidx])))
                
                del mxseid
                
            else:
                # no P and S detection in this time range
                assert((len(Pevidx) == 0) and (len(Sevidx) == 0))
                output_info[ista]['P']['detected'] = False  # recored the P phase detection status of this station
                output_info[ista]['S']['detected'] = False  # recored the S phase detection status of this station
                output_info[ista]['P']['sgname'] = None
                output_info[ista]['S']['sgname'] = None
            
            del Pevidx, Sevidx

        del ista
        
        # determine the earliest starttime of all the remaining detections
        for ista in stations:
            temp_pt = np.array(event_info[ista]['P']['starttime'])[np.array(event_info[ista]['P']['starttime']) > etime_amax]
            temp_st = np.array(event_info[ista]['S']['starttime'])[np.array(event_info[ista]['S']['starttime']) > etime_amax]
            if (len(temp_pt) > 0) and (len(temp_st) > 0):
                etime_sta.append(copy.deepcopy(min(min(temp_pt), min(temp_st))))
            elif (len(temp_pt) > 0) and (len(temp_st) == 0):
                etime_sta.append(copy.deepcopy(min(temp_pt)))
            elif (len(temp_pt) == 0) and (len(temp_st) > 0):
                etime_sta.append(copy.deepcopy(min(temp_st)))
                
            del temp_pt, temp_st
        del ista
        
        # determine if the event is locatable, 
        # i.e. detected by many stations (>= nsta_thrd), have enough phases (>= npha_thrd)
        if (nsta_trig >= nsta_thrd) and (npha_trig >= npha_thrd):
            # have enough triggered stations, output data set
            
            # determine the extending time window length
            twl_evres = twind_srch - (UTCDateTime(etime_amax) - UTCDateTime(srchtime_start)) 
            if twl_evres > 0:
                twlex_a = twl_evres + twlex
            else:
                twlex_a = copy.deepcopy(twlex)
                
            tt1 = srchtime_start - datetime.timedelta(seconds=twlex_a)  # the starttime of data extraction
            tt2 = etime_amax + datetime.timedelta(seconds=twlex_a)  # the endtime of data extraction
            output_info['tt1'] = copy.deepcopy(tt1)
            output_info['tt2'] = copy.deepcopy(tt2)
            
            # print info and output to file
            print('----------------------------------------------------------')
            print('Detect event at time range:', tt1, '-', tt2)
            print(nsta_trig, 'stations are triggered.')
            print(npha_trig, 'phases are detected.')
            print('Start to output data in this time range.')
            fepinfo.write(tt1.isoformat() + '    ' + tt2.isoformat() + '    ' + str(nsta_trig) + '    ' + str(npha_trig) + '\n')
            fepinfo.flush()
                        
            dir_output_ev = dir_output + '/' + tt1.isoformat()  # output directory of probability data for the current event/time_range
            os.makedirs(dir_output_ev, exist_ok=True)
            output_info['dir_output_ev'] = copy.deepcopy(dir_output_ev)
            
            if dir_seisdataset is not None:
                dir_output_seis_ev = dir_output_seis + '/' + tt1.isoformat()  # output directory of seismic data for the current event/time_range
                os.makedirs(dir_output_seis_ev, exist_ok=True)
                output_info['dir_output_seis_ev'] = copy.deepcopy(dir_output_seis_ev)
                del dir_output_seis_ev 
            
            # put information of the current detection into list
            detections_all.append(copy.deepcopy(output_info)) 
            
            del twl_evres, twlex_a, tt1, tt2, dir_output_ev
            
        if (len(etime_sta) > 0):
            srchtime_start = copy.deepcopy(min(etime_sta)) # update the start of the searched time range
        else:
            break
        
        del srchtime_end, nsta_trig, npha_trig, etime_sta, output_info, etime_amax    
    
    fepinfo.close()  
        
    # loop over each station and output data for each detected events
    # for each station only need to load data once
    for ista in stations:
        # load probability data set for the current station
        dataformat = event_info[ista]['filename'].split('.')[-1].lower()
        if dataformat == 'hdf5':
            datainfo['dt'] = copy.deepcopy(dt_EQT)  # data temporal sampling rate in second of the phase probability function
            pbdf = h5py.File(event_info[ista]['filename'], 'r')
            dsg_name = list(pbdf['probabilities'].keys())  # get the names of all probability data segments 
            dsg_starttime = np.array([datetime.datetime.strptime(idsgnm.split('_')[-1], dtformat_EQT) for idsgnm in dsg_name])  # get the starttimes of all probability data segments 
            dsg_endtime = np.array([iitime + datetime.timedelta(seconds=data_sglength_EQT) for iitime in dsg_starttime])  # get the endtimes of all probability data segments
        else:
            # inputs are MSEED format which can be read by obspy
            pbdf = obspy.read(event_info[ista]['filename'])
            pbdf.merge(method=1, fill_value=0, interpolation_samples=-1)
            datainfo['dt'] = pbdf[0].stats.delta  # data temporal sampling rate in second of the phase probability function
            dsg_name = [None,]
            dsg_starttime = np.array([pbdf[0].stats.starttime,])      
            dsg_endtime = np.array([pbdf[0].stats.endtime,])
        
        if dir_seisdataset is not None:
            # load all seismic data from the specified data folder of the current station
            dir_seismic_sta = os.path.join(dir_seisdataset, ista)
            assert(os.path.exists(dir_seismic_sta))
            stream = read_seismic_fromfd(dir_seismic_sta, channels=seismic_channels)
            # stream.merge(fill_value=0)
        
        for output_info in detections_all:
            # loop over each detected event
            tt1 = copy.deepcopy(output_info['tt1'])
            tt2 = copy.deepcopy(output_info['tt2'])
            dir_output_ev = copy.deepcopy(output_info['dir_output_ev'])
            
            if output_allsta or ((output_info[ista]['P']['detected'] or output_info[ista]['S']['detected'])):
                datainfo['station_name'] = copy.deepcopy(ista)
    
                if output_info[ista]['P']['sgname'] is not None:
                    # segment name of output data are already assigned
                    
                    # output for P probabilities
                    csg_indx = copy.deepcopy(dsg_name.index(output_info[ista]['P']['sgname']))  # the index of the chosen data segment for P
                    assert(output_info[ista]['P']['sgname'] == dsg_name[csg_indx])
                    if (dsg_starttime[csg_indx] - datetime.timedelta(seconds=dt_EQT) < tt1) and (dsg_endtime[csg_indx] + datetime.timedelta(seconds=dt_EQT) > tt2):
                        # the event data set is within one data segment
                        data_times = np.array([dsg_starttime[csg_indx] + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point for the chosen data segment
                        data_pdindex = np.logical_and((data_times >= tt1), (data_times <= tt2))  # the index of probability data point within the detection time range
                        odata_time = copy.deepcopy(data_times[data_pdindex])  # the timestampe of output data
                        datainfo['starttime'] = copy.deepcopy(odata_time[0])  # the starttime of the output data
                        
                        pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
                        pbdf['probabilities'][dsg_name[csg_indx]].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
                        oprob = copy.deepcopy(pbdata[data_pdindex,1])  # P-phase picking probability
                        
                        # output P-phase picking probability
                        datainfo['channel_name'] = 'PBP'  # note maximum three characters, the last one must be 'P'
                        vector2trace(datainfo, oprob, dir_output_ev)
                        del data_times, data_pdindex, odata_time, pbdata, oprob
                    
                    elif (dsg_starttime[csg_indx] - datetime.timedelta(seconds=dt_EQT) >= tt1) and (dsg_endtime[csg_indx] + datetime.timedelta(seconds=dt_EQT) > tt2):  
                        # starttime of event time range is earlier than the starttime of the chosen data segment
                        assert(dsg_starttime[csg_indx] <= tt2)
                        
                        # current segment
                        pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
                        pbdf['probabilities'][dsg_name[csg_indx]].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
                        data_times = np.array([dsg_starttime[csg_indx] + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point for the chosen data segment
                        data_pdindex = np.logical_and((data_times >= tt1), (data_times <= tt2))  # the index of probability data point within the detection time range
                        odata_time = copy.deepcopy(data_times[data_pdindex])  # the timestampe of output data
                        oprob_P = copy.deepcopy(pbdata[data_pdindex,1])  # P-phase picking probability
                        del pbdata, data_times, data_pdindex
                        assert(odata_time[0] >= tt1)
                        assert(odata_time[-1] <= tt2)
                        
                        # previous segment
                        if csg_indx > 0:
                            pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
                            pbdf['probabilities'][dsg_name[csg_indx-1]].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
                            data_times = np.array([dsg_starttime[csg_indx-1] + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point for the chosen data segment
                            data_pdindex = np.logical_and((data_times >= tt1), (data_times < dsg_starttime[csg_indx]))  # the index of probability data point within the detection time range
                            odata_time_a = copy.deepcopy(data_times[data_pdindex])  # the timestampe of output data
                            oprob_P_a = copy.deepcopy(pbdata[data_pdindex,1])  # P-phase picking probability
                            del pbdata, data_times, data_pdindex
                            assert(odata_time_a[0] >= tt1)
                            assert(odata_time_a[-1] <= tt2)
                            assert((odata_time[0] - odata_time_a[-1]).total_seconds() == dt_EQT)
                            
                            oprob = copy.deepcopy(np.concatenate((oprob_P_a, oprob_P)))
                            datainfo['starttime'] = copy.deepcopy(odata_time_a[0])
                            del odata_time_a, oprob_P_a, oprob_P, odata_time    
                        else:
                            # csg_indx = 0, first segment, no previous one
                            oprob = copy.deepcopy(oprob_P)
                            datainfo['starttime'] = copy.deepcopy(odata_time[0])
                            del odata_time, oprob_P
                        
                        # output phase probability
                        datainfo['channel_name'] = 'PBP'  # note maximum three characters, the last one must be 'P'
                        vector2trace(datainfo, oprob, dir_output_ev)
                        del oprob
                    
                    elif (dsg_starttime[csg_indx] - datetime.timedelta(seconds=dt_EQT) < tt1) and (dsg_endtime[csg_indx] + datetime.timedelta(seconds=dt_EQT) <= tt2):
                        # endtime of event time range is later than the endtime of the chosen data segment
                        assert(dsg_endtime[csg_indx] >= tt1)
                        
                        # current segment
                        pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
                        pbdf['probabilities'][dsg_name[csg_indx]].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
                        data_times = np.array([dsg_starttime[csg_indx] + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point for the chosen data segment
                        data_pdindex = np.logical_and((data_times >= tt1), (data_times <= tt2))  # the index of probability data point within the detection time range
                        odata_time = copy.deepcopy(data_times[data_pdindex])  # the timestampe of output data
                        oprob_P = copy.deepcopy(pbdata[data_pdindex,1])  # P-phase picking probability
                        del pbdata, data_times, data_pdindex
                        assert(odata_time[0] >= tt1)
                        assert(odata_time[-1] <= tt2)
                        
                        # after segment
                        if (csg_indx < len(dsg_name)-1):
                            pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
                            pbdf['probabilities'][dsg_name[csg_indx+1]].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
                            data_times = np.array([dsg_starttime[csg_indx+1] + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point for the chosen data segment
                            data_pdindex = np.logical_and((data_times > dsg_endtime[csg_indx]), (data_times <= tt2))  # the index of probability data point within the detection time range
                            odata_time_a = copy.deepcopy(data_times[data_pdindex])  # the timestampe of output data
                            oprob_P_a = copy.deepcopy(pbdata[data_pdindex,1])  # P-phase picking probability
                            del pbdata, data_times, data_pdindex
                            assert(odata_time_a[0] >= tt1)
                            assert(odata_time_a[-1] <= tt2)
                            assert((odata_time_a[0] - odata_time[-1]).total_seconds() == dt_EQT)
                            
                            oprob = copy.deepcopy(np.concatenate((oprob_P, oprob_P_a)))
                            datainfo['starttime'] = copy.deepcopy(odata_time[0])
                            del odata_time_a, oprob_P_a, oprob_P, odata_time
                            
                        else:
                            # csg_indx = len(dsg_name)-1, last segment, no after one
                            oprob = copy.deepcopy(oprob_P)
                            datainfo['starttime'] = copy.deepcopy(odata_time[0])
                            del odata_time, oprob_P
                        
                        # output phase probability
                        datainfo['channel_name'] = 'PBP'  # note maximum three characters, the last one must be 'P'
                        vector2trace(datainfo, oprob, dir_output_ev)
                        del oprob   
                    
                    elif (dsg_starttime[csg_indx] - datetime.timedelta(seconds=dt_EQT) >= tt1) and (dsg_endtime[csg_indx] + datetime.timedelta(seconds=dt_EQT) <= tt2):
                        # starttime of event time range is earlier than the starttime of the chosen data segment &
                        # and endtime of event time range is later than the endtime of the chosen data segment
                        assert(dsg_starttime[csg_indx] <= tt2)
                        assert(dsg_endtime[csg_indx] >= tt1)
                        
                        # current segment
                        pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
                        pbdf['probabilities'][dsg_name[csg_indx]].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
                        data_times = np.array([dsg_starttime[csg_indx] + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point for the chosen data segment
                        data_pdindex = np.logical_and((data_times >= tt1), (data_times <= tt2))  # the index of probability data point within the detection time range
                        odata_time = copy.deepcopy(data_times[data_pdindex])  # the timestampe of output data
                        oprob_P = copy.deepcopy(pbdata[data_pdindex,1])  # P-phase picking probability
                        del pbdata, data_times, data_pdindex
                        assert(odata_time[0] >= tt1)
                        assert(odata_time[-1] <= tt2)
                        
                        # previous segment
                        if csg_indx > 0:
                            pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
                            pbdf['probabilities'][dsg_name[csg_indx-1]].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
                            data_times = np.array([dsg_starttime[csg_indx-1] + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point for the chosen data segment
                            data_pdindex = np.logical_and((data_times >= tt1), (data_times < dsg_starttime[csg_indx]))  # the index of probability data point within the detection time range
                            odata_time_a = copy.deepcopy(data_times[data_pdindex])  # the timestampe of output data
                            oprob_P_a = copy.deepcopy(pbdata[data_pdindex,1])  # P-phase picking probability
                            del pbdata, data_times, data_pdindex
                            assert(odata_time_a[0] >= tt1)
                            assert(odata_time_a[-1] <= tt2)
                            assert((odata_time[0] - odata_time_a[-1]).total_seconds() == dt_EQT)
                            
                            oprob = copy.deepcopy(np.concatenate((oprob_P_a, oprob_P)))
                            datainfo['starttime'] = copy.deepcopy(odata_time_a[0])
                            del odata_time_a, oprob_P_a, oprob_P    
                        else:
                            # csg_indx = 0, first segment, no previous one
                            oprob = copy.deepcopy(oprob_P)
                            datainfo['starttime'] = copy.deepcopy(odata_time[0])
                            del oprob_P
                            
                        # after segment
                        if (csg_indx < len(dsg_name)-1):
                            pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
                            pbdf['probabilities'][dsg_name[csg_indx+1]].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
                            data_times = np.array([dsg_starttime[csg_indx+1] + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point for the chosen data segment
                            data_pdindex = np.logical_and((data_times > dsg_endtime[csg_indx]), (data_times <= tt2))  # the index of probability data point within the detection time range
                            odata_time_a = copy.deepcopy(data_times[data_pdindex])  # the timestampe of output data
                            oprob_P_a = copy.deepcopy(pbdata[data_pdindex,1])  # P-phase picking probability
                            del pbdata, data_times, data_pdindex
                            assert(odata_time_a[0] >= tt1)
                            assert(odata_time_a[-1] <= tt2)
                            assert((odata_time_a[0] - odata_time[-1]).total_seconds() == dt_EQT)
                            
                            oprob = copy.deepcopy(np.concatenate((oprob, oprob_P_a)))
                            del odata_time_a, oprob_P_a, odata_time   
                        else:
                            # csg_indx = len(dsg_name)-1, last segment, no after one
                            del odata_time    
                    
                        # output phase probability
                        datainfo['channel_name'] = 'PBP'  # note maximum three characters, the last one must be 'P'
                        vector2trace(datainfo, oprob, dir_output_ev)
                        del oprob
                        
                    del csg_indx
                    
                    # output for S probabilities
                    csg_indx = dsg_name.index(output_info[ista]['S']['sgname'])  # the index of the chosen data segment for S
                    assert(output_info[ista]['S']['sgname'] == dsg_name[csg_indx])
                    if (dsg_starttime[csg_indx] - datetime.timedelta(seconds=dt_EQT) < tt1) and (dsg_endtime[csg_indx] + datetime.timedelta(seconds=dt_EQT) > tt2):
                        # the event data set is within one data segment
                        data_times = np.array([dsg_starttime[csg_indx] + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point for the chosen data segment
                        data_pdindex = np.logical_and((data_times >= tt1), (data_times <= tt2))  # the index of probability data point within the detection time range
                        odata_time = copy.deepcopy(data_times[data_pdindex])  # the timestampe of output data
                        datainfo['starttime'] = copy.deepcopy(odata_time[0])  # the starttime of the output data
                        
                        pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
                        pbdf['probabilities'][dsg_name[csg_indx]].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
                        oprob = copy.deepcopy(pbdata[data_pdindex,2])  # S-phase picking probability
                        
                        # output S-phase picking probability
                        datainfo['channel_name'] = 'PBS'  # note maximum three characters, the last one must be 'S'
                        vector2trace(datainfo, oprob, dir_output_ev)
                        del data_times, data_pdindex, odata_time, pbdata, oprob
                    
                    elif (dsg_starttime[csg_indx] - datetime.timedelta(seconds=dt_EQT) >= tt1) and (dsg_endtime[csg_indx] + datetime.timedelta(seconds=dt_EQT) > tt2):  
                        # starttime of event time range is earlier than the starttime of the chosen data segment
                        assert(dsg_starttime[csg_indx] <= tt2)
                        
                        # current segment
                        pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
                        pbdf['probabilities'][dsg_name[csg_indx]].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
                        data_times = np.array([dsg_starttime[csg_indx] + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point for the chosen data segment
                        data_pdindex = np.logical_and((data_times >= tt1), (data_times <= tt2))  # the index of probability data point within the detection time range
                        odata_time = copy.deepcopy(data_times[data_pdindex])  # the timestampe of output data
                        oprob_S = copy.deepcopy(pbdata[data_pdindex,2])  # S-phase picking probability
                        del pbdata, data_times, data_pdindex
                        assert(odata_time[0] >= tt1)
                        assert(odata_time[-1] <= tt2)
                        
                        # previous segment
                        if csg_indx > 0:
                            pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
                            pbdf['probabilities'][dsg_name[csg_indx-1]].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
                            data_times = np.array([dsg_starttime[csg_indx-1] + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point for the chosen data segment
                            data_pdindex = np.logical_and((data_times >= tt1), (data_times < dsg_starttime[csg_indx]))  # the index of probability data point within the detection time range
                            odata_time_a = copy.deepcopy(data_times[data_pdindex])  # the timestampe of output data
                            oprob_S_a = copy.deepcopy(pbdata[data_pdindex,2])  # S-phase picking probability
                            del pbdata, data_times, data_pdindex
                            assert(odata_time_a[0] >= tt1)
                            assert(odata_time_a[-1] <= tt2)
                            assert((odata_time[0] - odata_time_a[-1]).total_seconds() == dt_EQT)
                            
                            oprob = copy.deepcopy(np.concatenate((oprob_S_a, oprob_S)))
                            datainfo['starttime'] = copy.deepcopy(odata_time_a[0])
                            del odata_time_a, oprob_S_a, oprob_S, odata_time    
                        else:
                            # csg_indx = 0, first segment, no previous one
                            oprob = copy.deepcopy(oprob_S)
                            datainfo['starttime'] = copy.deepcopy(odata_time[0])
                            del odata_time, oprob_S
                        
                        # output phase probability
                        datainfo['channel_name'] = 'PBS'  # note maximum three characters, the last one must be 'S'
                        vector2trace(datainfo, oprob, dir_output_ev)
                        del oprob
                    
                    elif (dsg_starttime[csg_indx] - datetime.timedelta(seconds=dt_EQT) < tt1) and (dsg_endtime[csg_indx] + datetime.timedelta(seconds=dt_EQT) <= tt2):
                        # endtime of event time range is later than the endtime of the chosen data segment
                        assert(dsg_endtime[csg_indx] >= tt1)
                        
                        # current segment
                        pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
                        pbdf['probabilities'][dsg_name[csg_indx]].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
                        data_times = np.array([dsg_starttime[csg_indx] + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point for the chosen data segment
                        data_pdindex = np.logical_and((data_times >= tt1), (data_times <= tt2))  # the index of probability data point within the detection time range
                        odata_time = copy.deepcopy(data_times[data_pdindex])  # the timestampe of output data
                        oprob_S = copy.deepcopy(pbdata[data_pdindex,2])  # S-phase picking probability
                        del pbdata, data_times, data_pdindex
                        assert(odata_time[0] >= tt1)
                        assert(odata_time[-1] <= tt2)
                        
                        # after segment
                        if (csg_indx < len(dsg_name)-1):
                            pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
                            pbdf['probabilities'][dsg_name[csg_indx+1]].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
                            data_times = np.array([dsg_starttime[csg_indx+1] + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point for the chosen data segment
                            data_pdindex = np.logical_and((data_times > dsg_endtime[csg_indx]), (data_times <= tt2))  # the index of probability data point within the detection time range
                            odata_time_a = copy.deepcopy(data_times[data_pdindex])  # the timestampe of output data
                            oprob_S_a = copy.deepcopy(pbdata[data_pdindex,2])  # S-phase picking probability
                            del pbdata, data_times, data_pdindex
                            assert(odata_time_a[0] >= tt1)
                            assert(odata_time_a[-1] <= tt2)
                            assert((odata_time_a[0] - odata_time[-1]).total_seconds() == dt_EQT)
                            
                            oprob = copy.deepcopy(np.concatenate((oprob_S, oprob_S_a)))
                            datainfo['starttime'] = copy.deepcopy(odata_time[0])
                            del odata_time_a, oprob_S_a, oprob_S, odata_time    
                        else:
                            # csg_indx = len(dsg_name)-1, last segment, no after one
                            oprob = copy.deepcopy(oprob_S)
                            datainfo['starttime'] = copy.deepcopy(odata_time[0])
                            del odata_time, oprob_S
                        
                        # output phase probability
                        datainfo['channel_name'] = 'PBS'  # note maximum three characters, the last one must be 'S'
                        vector2trace(datainfo, oprob, dir_output_ev)
                        del oprob 
                    
                    elif (dsg_starttime[csg_indx] - datetime.timedelta(seconds=dt_EQT) >= tt1) and (dsg_endtime[csg_indx] + datetime.timedelta(seconds=dt_EQT) <= tt2):
                        # starttime of event time range is earlier than the starttime of the chosen data segment &
                        # and endtime of event time range is later than the endtime of the chosen data segment
                        assert(dsg_starttime[csg_indx] <= tt2)
                        assert(dsg_endtime[csg_indx] >= tt1)
                        
                        # current segment
                        pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
                        pbdf['probabilities'][dsg_name[csg_indx]].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
                        data_times = np.array([dsg_starttime[csg_indx] + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point for the chosen data segment
                        data_pdindex = np.logical_and((data_times >= tt1), (data_times <= tt2))  # the index of probability data point within the detection time range
                        odata_time = copy.deepcopy(data_times[data_pdindex])  # the timestampe of output data
                        oprob_S = copy.deepcopy(pbdata[data_pdindex,2])  # S-phase picking probability
                        del pbdata, data_times, data_pdindex
                        assert(odata_time[0] >= tt1)
                        assert(odata_time[-1] <= tt2)
                        
                        # previous segment
                        if csg_indx > 0:
                            pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
                            pbdf['probabilities'][dsg_name[csg_indx-1]].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
                            data_times = np.array([dsg_starttime[csg_indx-1] + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point for the chosen data segment
                            data_pdindex = np.logical_and((data_times >= tt1), (data_times < dsg_starttime[csg_indx]))  # the index of probability data point within the detection time range
                            odata_time_a = copy.deepcopy(data_times[data_pdindex])  # the timestampe of output data
                            oprob_S_a = copy.deepcopy(pbdata[data_pdindex,2])  # S-phase picking probability
                            del pbdata, data_times, data_pdindex
                            assert(odata_time_a[0] >= tt1)
                            assert(odata_time_a[-1] <= tt2)
                            assert((odata_time[0] - odata_time_a[-1]).total_seconds() == dt_EQT)
                            
                            oprob = copy.deepcopy(np.concatenate((oprob_S_a, oprob_S)))
                            datainfo['starttime'] = copy.deepcopy(odata_time_a[0])
                            del odata_time_a, oprob_S_a, oprob_S    
                        else:
                            # csg_indx = 0, first segment, no previous one
                            oprob = copy.deepcopy(oprob_S)
                            datainfo['starttime'] = copy.deepcopy(odata_time[0])
                            del oprob_S
                            
                        # after segment
                        if (csg_indx < len(dsg_name)-1):
                            pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
                            pbdf['probabilities'][dsg_name[csg_indx+1]].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
                            data_times = np.array([dsg_starttime[csg_indx+1] + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point for the chosen data segment
                            data_pdindex = np.logical_and((data_times > dsg_endtime[csg_indx]), (data_times <= tt2))  # the index of probability data point within the detection time range
                            odata_time_a = copy.deepcopy(data_times[data_pdindex])  # the timestampe of output data
                            oprob_S_a = copy.deepcopy(pbdata[data_pdindex,2])  # S-phase picking probability
                            del pbdata, data_times, data_pdindex
                            assert(odata_time_a[0] >= tt1)
                            assert(odata_time_a[-1] <= tt2)
                            assert((odata_time_a[0] - odata_time[-1]).total_seconds() == dt_EQT)
                            
                            oprob = copy.deepcopy(np.concatenate((oprob, oprob_S_a)))
                            del odata_time_a, oprob_S_a, odata_time   
                        else:
                            # csg_indx = len(dsg_name)-1, last segment, no after one
                            del odata_time    
                    
                        # output phase probability
                        datainfo['channel_name'] = 'PBS'  # note maximum three characters, the last one must be 'S'
                        vector2trace(datainfo, oprob, dir_output_ev)
                        del oprob
                    
                    del csg_indx
                    
                else:
                    # segment name not assigned, determine according to 'tt1' and 'tt2' 
                    assert((output_info[ista]['P']['sgname'] is None) and (output_info[ista]['S']['sgname'] is None))
                    
                    if dataformat == 'hdf5':
                        # input are hdf5 data
                        dindx = np.logical_and((dsg_starttime <= tt1), (dsg_endtime >= tt2))  # the index of data segments that include the whole event time period: tt1 - tt2
                        if dindx.any():
                            # have data segments that fulfill the requirements
                            # find the data segment where the event time period is mostly around the center
                            tt_mid =  tt1 + (tt2 - tt1)/2  # the midpoint between the starttime and endtime of data extraction
                            mdtimesdf = np.array([ttdfc.total_seconds() for ttdfc in (dsg_starttime[dindx] + datetime.timedelta(seconds=0.5*data_sglength_EQT) - tt_mid)])  # time difference in second between the midpoint of the fulfilled data segments time range and the event time period
                            data_sgindex = np.flatnonzero(dindx)[np.argmin(abs(mdtimesdf))]  # the index of the chosen data segment (closest to the middle time), is an integer
                            data_sgname = dsg_name[data_sgindex]  # the segment name of the chosen data segment
                            data_starttime = dsg_starttime[data_sgindex]  # the starttime of the chosen data segment
                            data_times = np.array([data_starttime + datetime.timedelta(seconds=iitp*dt_EQT) for iitp in range(data_size_EQT)])  # timestampe of each data point for the chosen data segment
                            data_pdindex = np.logical_and((data_times >= tt1), (data_times <= tt2))  # the index of probability data point within the detection time range
                            odata_time = copy.deepcopy(data_times[data_pdindex])  # the timestampe of output data
                            datainfo['starttime'] = copy.deepcopy(odata_time[0])  # the starttime of the output data
                            datainfo['dt'] = copy.deepcopy(dt_EQT)
                            pbdata = np.zeros((data_size_EQT, 3), dtype=np.float32)  # initialize array for load prob data set
                            pbdf['probabilities'][data_sgname].read_direct(pbdata)  # EQT probability data set, shape: 6000*3
                            oprob_P = pbdata[data_pdindex,1]  # P-phase picking probability
                            oprob_S = pbdata[data_pdindex,2]  # S-phase picking probability
                                
                            # output P-phase picking probability
                            datainfo['channel_name'] = 'PBP'  # note maximum three characters, the last one must be 'P'
                            vector2trace(datainfo, oprob_P, dir_output_ev)
                            
                            # output S-phase picking probability
                            datainfo['channel_name'] = 'PBS'  # note maximum three characters, the last one must be 'S'
                            vector2trace(datainfo, oprob_S, dir_output_ev)
                            
                            del tt_mid, mdtimesdf, data_sgindex, data_sgname, data_starttime, data_times 
                            del data_pdindex, odata_time, pbdata, oprob_P, oprob_S
                        del dindx
                    else:
                        # input are mseed data
                        for jjtr in pbdf:
                            assert(jjtr.stats.starttime == dsg_starttime[0])
                            assert(jjtr.stats.endtime == dsg_endtime[0])
                            if jjtr.stats.channel[-1].upper() == 'P':
                                # cut and save P_prob
                                trprob = jjtr.slice(starttime=tt1, endtime=tt2, nearest_sample=True)
                                if trprob.stats.npts > 0:
                                    if trprob.id[0] == '.':
                                        nametag = trprob.id[1:] + '.' + trprob.stats.starttime.strftime(timeformat) + '.mseed'
                                    else:
                                        nametag = trprob.id + '.' + trprob.stats.starttime.strftime(timeformat) + '.mseed'
                                    evpbfname = os.path.join(dir_output_ev, nametag)
                                    trprob.write(evpbfname, format="MSEED")
                            elif jjtr.stats.channel[-1].upper() == 'S':
                                # cut and save S_prob
                                trprob = jjtr.slice(starttime=tt1, endtime=tt2, nearest_sample=True)
                                if trprob.stats.npts > 0:
                                    if trprob.id[0] == '.':
                                        nametag = trprob.id[1:] + '.' + trprob.stats.starttime.strftime(timeformat) + '.mseed'
                                    else:
                                        nametag = trprob.id + '.' + trprob.stats.starttime.strftime(timeformat) + '.mseed'
                                    evpbfname = os.path.join(dir_output_ev, nametag)
                                    trprob.write(evpbfname, format="MSEED")
                            
                # check if need to output raw seismic data segments for the detected event
                if dir_seisdataset is not None:
                    # output raw seismic data segment for the detected event
                    output_seissegment(stream, output_info['dir_output_seis_ev'], tt1, tt2)
            
            del tt1, tt2, dir_output_ev
            
        del pbdf, dsg_name, dsg_starttime, dsg_endtime 
        if dir_seisdataset is not None:
            del stream
        
    return




